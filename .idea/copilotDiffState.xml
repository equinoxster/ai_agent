<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="import os&#10;import sys&#10;from dotenv import load_dotenv&#10;from google import genai&#10;from google.genai import types&#10;&#10;from functions.get_files_info import schema_get_files_info&#10;from functions.get_file_content import schema_get_file_content&#10;from functions.run_python_file import schema_run_python_file&#10;from functions.write_file import schema_write_file&#10;from functions.call_function import call_function&#10;&#10;load_dotenv()&#10;api_key = os.environ.get(&quot;GEMINI_API_KEY&quot;)&#10;&#10;&#10;def main():&#10;    if len(sys.argv) &lt; 2 or sys.argv[1] is None:&#10;        print(&quot;Prompt value not found!&quot;)&#10;        sys.exit(1)&#10;&#10;    user_prompt = sys.argv[1]&#10;    verbose = False&#10;    for arg in sys.argv:&#10;        if arg.startswith(&quot;--&quot;):&#10;            match arg[2:]:&#10;                case &quot;verbose&quot;:&#10;                    verbose = True&#10;                case _:&#10;                    pass&#10;    client = genai.Client(api_key=api_key)&#10;&#10;    available_functions = types.Tool(&#10;        function_declarations=[&#10;            schema_get_files_info,&#10;            schema_get_file_content,&#10;            schema_run_python_file,&#10;            schema_write_file&#10;        ]&#10;    )&#10;&#10;&#10;    messages = [types.Content(role=&quot;user&quot;, parts=[types.Part(text=user_prompt)])]&#10;&#10;    system_prompt = &quot;&quot;&quot;&#10;    You are a helpful AI coding agent.&#10;&#10;    When a user asks a question or makes a request, make a function call plan. You can perform the following operations:&#10;&#10;    - List files and directories&#10;&#10;    All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.&#10;    &quot;&quot;&quot;&#10;&#10;    model_name = &quot;gemini-1.5-flash-latest&quot;&#10;    model_path = f&quot;models/{model_name}&quot;&#10;&#10;    for i in range(20):&#10;        try:&#10;            response = client.models.generate_content(&#10;                model=model_path,&#10;                contents=messages,&#10;                generation_config=types.GenerationConfig(&#10;                    temperature=0.0,&#10;                ),&#10;                tools=[available_functions],&#10;                system_instruction=system_prompt,&#10;            )&#10;&#10;            if response.text:&#10;                print(response.text)&#10;                break&#10;&#10;            if response.candidates[0].finish_reason.name == &quot;TOOL_USE&quot;:&#10;                messages.append(response.candidates[0].content)&#10;                function_responses = []&#10;                for function_call in response.candidates[0].content.parts:&#10;                    if function_call.function_call:&#10;                        result = call_function(function_call.function_call, verbose)&#10;                        function_responses.append(result)&#10;                messages.append(&#10;                    types.Content(&#10;                        role=&quot;tool&quot;,&#10;                        parts=function_responses,&#10;                    )&#10;                )&#10;            else:&#10;                break&#10;        except Exception as e:&#10;            print(f&quot;An error occurred: {e}&quot;)&#10;            break&#10;    else:&#10;        print(&quot;Max iterations reached.&quot;)&#10;&#10;    if verbose:&#10;        print(f&quot;User prompt: {user_prompt} &quot;)&#10;        print(f&quot;Prompt tokens: {response.usage_metadata.prompt_token_count}&quot;)&#10;        print(f&quot;Response tokens: {response.usage_metadata.candidates_token_count}&quot;)&#10;&#10;&#10;main()&#10;" />
              <option name="updatedContent" value="import os&#10;import sys&#10;from dotenv import load_dotenv&#10;from google import genai&#10;from google.genai import types&#10;&#10;from functions.get_files_info import schema_get_files_info&#10;from functions.get_file_content import schema_get_file_content&#10;from functions.run_python_file import schema_run_python_file&#10;from functions.write_file import schema_write_file&#10;from functions.call_function import call_function&#10;&#10;load_dotenv()&#10;api_key = os.environ.get(&quot;GEMINI_API_KEY&quot;)&#10;&#10;&#10;def main():&#10;    if len(sys.argv) &lt; 2 or sys.argv[1] is None:&#10;        print(&quot;Prompt value not found!&quot;)&#10;        sys.exit(1)&#10;&#10;    user_prompt = sys.argv[1]&#10;    verbose = False&#10;    for arg in sys.argv:&#10;        if arg.startswith(&quot;--&quot;):&#10;            match arg[2:]:&#10;                case &quot;verbose&quot;:&#10;                    verbose = True&#10;                case _:&#10;                    pass&#10;    client = genai.Client(api_key=api_key)&#10;&#10;    available_functions = types.Tool(&#10;        function_declarations=[&#10;            schema_get_files_info,&#10;            schema_get_file_content,&#10;            schema_run_python_file,&#10;            schema_write_file&#10;        ]&#10;    )&#10;&#10;&#10;    messages = [types.Content(role=&quot;user&quot;, parts=[types.Part(text=user_prompt)])]&#10;&#10;    system_prompt = &quot;&quot;&quot;&#10;    You are a helpful AI coding agent.&#10;&#10;    When a user asks a question or makes a request, make a function call plan. You can perform the following operations:&#10;&#10;    - List files and directories&#10;&#10;    All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.&#10;    &quot;&quot;&quot;&#10;&#10;    model_name = &quot;gemini-1.5-flash-latest&quot;&#10;    model_path = f&quot;models/{model_name}&quot;&#10;&#10;    for i in range(20):&#10;        try:&#10;            response = client.models.generate_content(&#10;                model=model_path,&#10;                contents=messages,&#10;                generation_config=types.GenerationConfig(&#10;                    temperature=0.0,&#10;                ),&#10;                tools=[available_functions],&#10;                system_instruction=system_prompt,&#10;            )&#10;&#10;            if response.text:&#10;                print(response.text)&#10;                break&#10;&#10;            if response.candidates[0].finish_reason.name == &quot;TOOL_USE&quot;:&#10;                messages.append(response.candidates[0].content)&#10;                function_responses = []&#10;                for function_call in response.candidates[0].content.parts:&#10;                    if function_call.function_call:&#10;                        result = call_function(function_call.function_call, verbose)&#10;                        function_responses.append(result)&#10;                messages.append(&#10;                    types.Content(&#10;                        role=&quot;tool&quot;,&#10;                        parts=function_responses,&#10;                    )&#10;                )&#10;            else:&#10;                break&#10;        except Exception as e:&#10;            print(f&quot;An error occurred: {e}&quot;)&#10;            break&#10;    else:&#10;        print(&quot;Max iterations reached.&quot;)&#10;&#10;    if verbose:&#10;        print(f&quot;User prompt: {user_prompt} &quot;)&#10;        print(f&quot;Prompt tokens: {response.usage_metadata.prompt_token_count}&quot;)&#10;        print(f&quot;Response tokens: {response.usage_metadata.candidates_token_count}&quot;)&#10;&#10;&#10;main()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>